{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchinfo import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    \"\"\"\n",
    "    输入Shape为(N, in_dim)，N为batch_size, in_dim是随机向量的维度\n",
    "    输出Shape为(N, 3, 64, 64)，即生成N张64x64的彩色图像\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_dim, dim=64):\n",
    "        super(Generator, self).__init__()\n",
    "\n",
    "        def dconv_bn_relu(in_dim, out_dim):\n",
    "            return nn.Sequential(\n",
    "                nn.ConvTranspose2d(in_dim, out_dim, 5, 2,\n",
    "                                   padding=2, output_padding=1, bias=False),\n",
    "                nn.BatchNorm2d(out_dim),\n",
    "                nn.ReLU()\n",
    "            )\n",
    "\n",
    "        # 1. 先用线性层将随机向量变成 dim*8 个通道，大小为4x4的图片\n",
    "        self.l1 = nn.Sequential(\n",
    "            nn.Linear(in_dim, dim * 8 * 4 * 4, bias=False),\n",
    "            nn.BatchNorm1d(dim * 8 * 4 * 4),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        # 2. 然后就一直反卷积，不断的将图片变大，同时通道不断减小，最终变成一个3通道，64x64大小的图片\n",
    "        self.l2_5 = nn.Sequential(\n",
    "            dconv_bn_relu(dim * 8, dim * 4),\n",
    "            dconv_bn_relu(dim * 4, dim * 2),\n",
    "            dconv_bn_relu(dim * 2, dim),\n",
    "            nn.ConvTranspose2d(dim, 3, 5, 2, padding=2, output_padding=1),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = self.l1(x)\n",
    "        y = y.view(y.size(0), -1, 4, 4)\n",
    "        y = self.l2_5(y)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\development\\miniconda\\lib\\site-packages\\torchinfo\\torchinfo.py:477: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  action_fn=lambda data: sys.getsizeof(data.storage()),\n",
      "d:\\development\\miniconda\\lib\\site-packages\\torch\\storage.py:665: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return super().__sizeof__() + self.nbytes()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "Generator                                [100, 3, 64, 64]          --\n",
       "├─Sequential: 1-1                        [100, 8192]               --\n",
       "│    └─Linear: 2-1                       [100, 8192]               819,200\n",
       "│    └─BatchNorm1d: 2-2                  [100, 8192]               16,384\n",
       "│    └─ReLU: 2-3                         [100, 8192]               --\n",
       "├─Sequential: 1-2                        [100, 3, 64, 64]          --\n",
       "│    └─Sequential: 2-4                   [100, 256, 8, 8]          --\n",
       "│    │    └─ConvTranspose2d: 3-1         [100, 256, 8, 8]          3,276,800\n",
       "│    │    └─BatchNorm2d: 3-2             [100, 256, 8, 8]          512\n",
       "│    │    └─ReLU: 3-3                    [100, 256, 8, 8]          --\n",
       "│    └─Sequential: 2-5                   [100, 128, 16, 16]        --\n",
       "│    │    └─ConvTranspose2d: 3-4         [100, 128, 16, 16]        819,200\n",
       "│    │    └─BatchNorm2d: 3-5             [100, 128, 16, 16]        256\n",
       "│    │    └─ReLU: 3-6                    [100, 128, 16, 16]        --\n",
       "│    └─Sequential: 2-6                   [100, 64, 32, 32]         --\n",
       "│    │    └─ConvTranspose2d: 3-7         [100, 64, 32, 32]         204,800\n",
       "│    │    └─BatchNorm2d: 3-8             [100, 64, 32, 32]         128\n",
       "│    │    └─ReLU: 3-9                    [100, 64, 32, 32]         --\n",
       "│    └─ConvTranspose2d: 2-7              [100, 3, 64, 64]          4,803\n",
       "│    └─Tanh: 2-8                         [100, 3, 64, 64]          --\n",
       "==========================================================================================\n",
       "Total params: 5,142,083\n",
       "Trainable params: 5,142,083\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (G): 64.97\n",
       "==========================================================================================\n",
       "Input size (MB): 0.04\n",
       "Forward/backward pass size (MB): 206.44\n",
       "Params size (MB): 20.57\n",
       "Estimated Total Size (MB): 227.05\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = Generator(in_dim=100)\n",
    "\n",
    "summary(net, (100, 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    \"\"\"\n",
    "    输入Shape为(N, 3, 64, 64)，即N张64x64的彩色图片\n",
    "    输出Shape为(N,), 即这N个图片每张图片的真实率，越接近1表示Discriminator越觉得它是真的\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_dim=3, dim=64): # 注意这里的in_dim是指的图片的通道数，所以是3\n",
    "        super(Discriminator, self).__init__()\n",
    "\n",
    "        def conv_bn_lrelu(in_dim, out_dim):\n",
    "            return nn.Sequential(\n",
    "                nn.Conv2d(in_dim, out_dim, 5, 2, 2),\n",
    "                nn.BatchNorm2d(out_dim),\n",
    "                nn.LeakyReLU(0.2),\n",
    "            )\n",
    "\n",
    "        # 就是一堆卷积一直卷，把原始的图片最终卷成一个数字\n",
    "        self.ls = nn.Sequential(\n",
    "            nn.Conv2d(in_dim, dim, 5, 2, 2),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            conv_bn_lrelu(dim, dim * 2),\n",
    "            conv_bn_lrelu(dim * 2, dim * 4),\n",
    "            conv_bn_lrelu(dim * 4, dim * 8),\n",
    "            nn.Conv2d(dim * 8, 1, 4),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = self.ls(x)\n",
    "        y = y.view(-1)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "Discriminator                            [100]                     --\n",
       "├─Sequential: 1-1                        [100, 1, 1, 1]            --\n",
       "│    └─Conv2d: 2-1                       [100, 64, 32, 32]         4,864\n",
       "│    └─LeakyReLU: 2-2                    [100, 64, 32, 32]         --\n",
       "│    └─Sequential: 2-3                   [100, 128, 16, 16]        --\n",
       "│    │    └─Conv2d: 3-1                  [100, 128, 16, 16]        204,928\n",
       "│    │    └─BatchNorm2d: 3-2             [100, 128, 16, 16]        256\n",
       "│    │    └─LeakyReLU: 3-3               [100, 128, 16, 16]        --\n",
       "│    └─Sequential: 2-4                   [100, 256, 8, 8]          --\n",
       "│    │    └─Conv2d: 3-4                  [100, 256, 8, 8]          819,456\n",
       "│    │    └─BatchNorm2d: 3-5             [100, 256, 8, 8]          512\n",
       "│    │    └─LeakyReLU: 3-6               [100, 256, 8, 8]          --\n",
       "│    └─Sequential: 2-5                   [100, 512, 4, 4]          --\n",
       "│    │    └─Conv2d: 3-7                  [100, 512, 4, 4]          3,277,312\n",
       "│    │    └─BatchNorm2d: 3-8             [100, 512, 4, 4]          1,024\n",
       "│    │    └─LeakyReLU: 3-9               [100, 512, 4, 4]          --\n",
       "│    └─Conv2d: 2-6                       [100, 1, 1, 1]            8,193\n",
       "│    └─Sigmoid: 2-7                      [100, 1, 1, 1]            --\n",
       "==========================================================================================\n",
       "Total params: 4,316,545\n",
       "Trainable params: 4,316,545\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (G): 16.23\n",
       "==========================================================================================\n",
       "Input size (MB): 4.92\n",
       "Forward/backward pass size (MB): 144.18\n",
       "Params size (MB): 17.27\n",
       "Estimated Total Size (MB): 166.36\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d_net = Discriminator()\n",
    "summary(d_net, (100, 3, 64, 64))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3310467ee498335a253e0baee96358203e0c88417c5604ba3a9ce8aa94f70c7f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
